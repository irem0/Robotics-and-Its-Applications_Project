{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrJPT3/2RyeXz0EDTdghYk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plxvIN_5hDfy"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Hyperparameters\n",
        "STATE_DIM = 6  # Acrobot state space dimension\n",
        "ACTION_DIM = 3  # Discrete action space: 3 actions\n",
        "MAX_EPISODES = 500\n",
        "MAX_TIMESTEPS = 200\n",
        "BATCH_SIZE = 64\n",
        "GAMMA = 0.99  # Discount factor\n",
        "TAU = 0.005  # Target network update rate\n",
        "LR = 1e-3  # Learning rate for the actor and critic\n",
        "EXPLORATION_NOISE = 0.1  # Noise added to actions for exploration\n",
        "BUFFER_SIZE = 100000  # Replay buffer size\n",
        "\n",
        "# Define Actor Network\n",
        "class Actor(nn.Module):\n",
        "    def _init_(self):\n",
        "        super(Actor, self)._init_()\n",
        "        self.fc1 = nn.Linear(STATE_DIM, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, ACTION_DIM)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        action = torch.tanh(self.fc3(x))  # Tanh to keep actions within [-1, 1]\n",
        "        return action\n",
        "\n",
        "# Define Critic Network\n",
        "class Critic(nn.Module):\n",
        "    def _init_(self):\n",
        "        super(Critic, self)._init_()\n",
        "        self.fc1 = nn.Linear(STATE_DIM + ACTION_DIM, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.relu(self.fc1(torch.cat([state, action], dim=-1)))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        q_value = self.fc3(x)\n",
        "        return q_value\n",
        "\n",
        "# Define the DDPG Agent\n",
        "class DDPGAgent:\n",
        "    def _init_(self, env):\n",
        "        self.env = env\n",
        "        self.actor = Actor().to(device)\n",
        "        self.target_actor = Actor().to(device)\n",
        "        self.critic = Critic().to(device)\n",
        "        self.target_critic = Critic().to(device)\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=LR)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=LR)\n",
        "\n",
        "        self.replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
        "\n",
        "        self.update_target_networks()\n",
        "\n",
        "    def update_target_networks(self):\n",
        "        # Soft update of target networks\n",
        "        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
        "            target_param.data.copy_(param.data)\n",
        "\n",
        "    def select_action(self, state, noise=0.0):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        action = self.actor(state).cpu().detach().numpy()[0]\n",
        "        return np.clip(action + noise * np.random.randn(ACTION_DIM), -1.0, 1.0)\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample_batch(self):\n",
        "        batch = random.sample(self.replay_buffer, BATCH_SIZE)\n",
        "        state, action, reward, next_state, done = zip(*batch)\n",
        "        return (\n",
        "            torch.FloatTensor(state).to(device),\n",
        "            torch.FloatTensor(action).to(device),\n",
        "            torch.FloatTensor(reward).to(device),\n",
        "            torch.FloatTensor(next_state).to(device),\n",
        "            torch.FloatTensor(done).to(device)\n",
        "        )\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.replay_buffer) < BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        state, action, reward, next_state, done = self.sample_batch()\n",
        "\n",
        "        # Train Critic\n",
        "        with torch.no_grad():\n",
        "            next_action = self.target_actor(next_state)\n",
        "            target_q = reward + GAMMA * (1 - done) * self.target_critic(next_state, next_action)\n",
        "\n",
        "        current_q = self.critic(state, action)\n",
        "        critic_loss = nn.MSELoss()(current_q, target_q)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Train Actor\n",
        "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Soft update target networks\n",
        "        self.update_target_networks()\n",
        "\n",
        "# Training Loop\n",
        "if _name_ == \"_main_\":\n",
        "    env = gym.make(\"Acrobot-v1\")\n",
        "    agent = DDPGAgent(env)\n",
        "\n",
        "    all_rewards = []\n",
        "    for episode in range(MAX_EPISODES):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        for t in range(MAX_TIMESTEPS):\n",
        "            action = agent.select_action(state, noise=EXPLORATION_NOISE)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.store_experience(state, action, reward, next_state, done)\n",
        "            agent.train()\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        all_rewards.append(total_reward)\n",
        "        print(f\"Episode {episode+1}/{MAX_EPISODES}, Reward: {total_reward}\")\n",
        "\n",
        "        # Plot rewards periodically\n",
        "        if episode % 50 == 0:\n",
        "            plt.plot(all_rewards)\n",
        "            plt.xlabel(\"Episode\")\n",
        "            plt.ylabel(\"Reward\")\n",
        "            plt.show()\n",
        "\n",
        "    # Close environment\n",
        "    env.close()"
      ]
    }
  ]
}