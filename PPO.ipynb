{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMq5LyQ0aoRmh2pp4ElATf/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmxCLeuZQgrN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import os\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "device = torch.device(\"cpu\")\n",
        "dtype = torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO_Network(nn.Module):\n",
        "    def __init__(self, in_channels, num_actions, continuous=False):\n",
        "        super().__init__()\n",
        "        self.continuous = continuous\n",
        "        self.num_actions = num_actions\n",
        "\n",
        "        # Ağ yapısı\n",
        "        network = [\n",
        "            nn.Linear(in_channels, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, num_actions + (1 if continuous else 1))\n",
        "        ]\n",
        "\n",
        "        self.network = nn.Sequential(*network)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.network(x)\n",
        "        if self.continuous:\n",
        "            mean, std = torch.split(output, [self.num_actions, 1], dim=1)\n",
        "            std = torch.clamp(F.softplus(std), min=1e-3)  # Küçük değerleri önlemek için std'yi sınırla\n",
        "            return mean, std\n",
        "        else:\n",
        "            policy, value = torch.split(output, [self.num_actions, 1], dim=1)\n",
        "            policy = F.softmax(policy, dim=1)  # Ayrık aksiyonlar için softmax\n",
        "            return policy, value\n",
        "\n",
        "\n",
        "class PPO_Agent(nn.Module):\n",
        "    def __init__(self, in_channels, num_actions, continuous=False):\n",
        "        super().__init__()\n",
        "        self.continuous = continuous\n",
        "        self.network = PPO_Network(in_channels, num_actions, continuous)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "    def select_action(self, output):\n",
        "        if self.continuous:\n",
        "            mean, std = output\n",
        "            dist = torch.distributions.Normal(mean, std)\n",
        "            action = dist.sample()\n",
        "            return action.detach().cpu().numpy(), dist.log_prob(action).sum(axis=-1)\n",
        "        else:\n",
        "            policy, _ = output\n",
        "            action = np.random.choice(range(self.num_actions), p=policy.detach().cpu().numpy()[0])\n",
        "            return action, torch.log(policy.squeeze(0)[action])\n"
      ],
      "metadata": {
        "id": "ooxankUPQtL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger:\n",
        "\n",
        "    def __init__(self, filename):\n",
        "        self.filename = filename\n",
        "        f = open(f\"{self.filename}.csv\", \"w\")\n",
        "        f.close()\n",
        "\n",
        "    def log(self, msg):\n",
        "        f = open(f\"{self.filename}.csv\", \"a+\")\n",
        "        f.write(f\"{msg}\\n\")\n",
        "        f.close()\n",
        "\n",
        "total_steps = 0\n",
        "class Env_Runner:\n",
        "\n",
        "    def __init__(self, env_name, agent, logger_folder):\n",
        "        super().__init__\n",
        "\n",
        "\n",
        "        self.env = gym.make(env_name)\n",
        "        self.agent = agent\n",
        "\n",
        "        self.logger = Logger(f'{logger_folder}/training_info')\n",
        "        self.logger.log(\"training_step, return\")\n",
        "\n",
        "        self.ob = self.env.reset()\n",
        "        self.Return = 0\n",
        "\n",
        "    def run(self, steps):\n",
        "        global total_steps\n",
        "\n",
        "        obs = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        dones = []\n",
        "        values = []\n",
        "        action_prob = []\n",
        "\n",
        "        for step in range(steps):\n",
        "\n",
        "            self.ob = torch.tensor(self.ob).to(device).to(dtype)\n",
        "            policy, value = self.agent(self.ob.unsqueeze(0))\n",
        "            action = self.agent.select_action(policy.detach().cpu().numpy()[0])\n",
        "\n",
        "            obs.append(self.ob)\n",
        "            actions.append(action)\n",
        "            values.append(value.detach())\n",
        "            action_prob.append(policy[0,action].detach())\n",
        "\n",
        "            self.ob, r, done, info = self.env.step(action)\n",
        "            self.Return += r\n",
        "\n",
        "            if done: # environment reset\n",
        "                self.ob = self.env.reset()\n",
        "                self.logger.log(f'{total_steps+step},{self.Return}')\n",
        "                print(\"Return:\",self.Return)\n",
        "                self.Return = 0\n",
        "\n",
        "            rewards.append(r)\n",
        "            dones.append(done)\n",
        "\n",
        "        total_steps += steps\n",
        "\n",
        "        return [obs, actions, rewards, dones, values, action_prob]"
      ],
      "metadata": {
        "id": "pb86cXQQQwGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99\n",
        "lam = 0.95\n",
        "def compute_advantage_and_value_targets(rewards, values, dones):\n",
        "\n",
        "    advantage_values = []\n",
        "    old_adv_t = torch.tensor(0.0).to(device)\n",
        "\n",
        "    value_targets = []\n",
        "    old_value_target = values[-1]\n",
        "\n",
        "    for t in reversed(range(len(rewards)-1)):\n",
        "\n",
        "        if dones[t]:\n",
        "            old_adv_t = torch.tensor(0.0).to(device)\n",
        "\n",
        "        # ADV\n",
        "        delta_t = rewards[t] + (gamma*(values[t+1])*int(not dones[t+1])) - values[t]\n",
        "\n",
        "        A_t = delta_t + gamma*lam*old_adv_t\n",
        "        advantage_values.append(A_t[0])\n",
        "\n",
        "        old_adv_t = delta_t + gamma*lam*old_adv_t\n",
        "\n",
        "        # VALUE TARGET\n",
        "        value_target = rewards[t] + gamma*old_value_target*int(not dones[t+1])\n",
        "        value_targets.append(value_target[0])\n",
        "\n",
        "        old_value_target = value_target\n",
        "\n",
        "    advantage_values.reverse()\n",
        "    value_targets.reverse()\n",
        "\n",
        "    return advantage_values, value_targets"
      ],
      "metadata": {
        "id": "tZFrrTWsQzEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Batch_DataSet(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, obs, actions, adv, v_t, old_action_prob):\n",
        "        super().__init__()\n",
        "        self.obs = obs\n",
        "        self.actions = actions\n",
        "        self.adv = adv\n",
        "        self.v_t = v_t\n",
        "        self.old_action_prob = old_action_prob\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.obs.shape[0]\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.obs[i],self.actions[i],self.adv[i],self.v_t[i],self.old_action_prob[i]"
      ],
      "metadata": {
        "id": "xY_J9ck6Q1Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create folder to save networks, csv, hyperparameter\n",
        "folder_name = time.asctime(time.gmtime()).replace(\" \",\"_\").replace(\":\",\"_\")\n",
        "os.mkdir(folder_name)\n",
        "\n",
        "envs = {\n",
        "    \"Acrobot-v1\": {\"continuous\": False},\n",
        "    \"MountainCarContinuous-v0\": {\"continuous\": True}\n",
        "}\n",
        "env = gym.make(envs)\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "epochs = 4\n",
        "T = 65\n",
        "minibatch_size = 32\n",
        "lr = 1e-3\n",
        "eps = 0.1\n",
        "c1 = 0.1\n",
        "\n",
        "agent = PPO_Agent(obs_dim, num_actions).to(device)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=lr)\n",
        "actors = 4\n",
        "env_runners = [Env_Runner(envs, agent, folder_name) for i in range(actors)]"
      ],
      "metadata": {
        "id": "H3PLA6SyQ3Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterations = 1000\n",
        "for i in range(iterations):\n",
        "\n",
        "    # get data\n",
        "    batch_obs, batch_actions, batch_adv, batch_v_t, batch_old_action_prob = None, None, None, None, None\n",
        "\n",
        "    for env_runner in env_runners:\n",
        "        obs, actions, rewards, dones, values, old_action_prob = env_runner.run(T)\n",
        "        adv, v_t = compute_advantage_and_value_targets(rewards, values, dones)\n",
        "\n",
        "        # assemble data from the different runners\n",
        "        batch_obs = torch.stack(obs[:-1]) if batch_obs == None else torch.cat([batch_obs,torch.stack(obs[:-1])])\n",
        "        batch_actions = np.stack(actions[:-1]) if batch_actions is None else np.concatenate([batch_actions,np.stack(actions[:-1])])\n",
        "        batch_adv = torch.stack(adv) if batch_adv == None else torch.cat([batch_adv,torch.stack(adv)])\n",
        "        batch_v_t = torch.stack(v_t) if batch_v_t == None else torch.cat([batch_v_t,torch.stack(v_t)])\n",
        "        batch_old_action_prob = torch.stack(old_action_prob[:-1]) if batch_old_action_prob == None else torch.cat([batch_old_action_prob,torch.stack(old_action_prob[:-1])])\n",
        "\n",
        "    # load into dataset/loader\n",
        "    dataset = Batch_DataSet(batch_obs,batch_actions,batch_adv,batch_v_t,batch_old_action_prob)\n",
        "    dataloader = DataLoader(dataset, batch_size=minibatch_size, num_workers=0, shuffle=True)\n",
        "\n",
        "    # update\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # sample minibatches\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # get data\n",
        "            obs, actions, adv, v_target, old_action_prob = batch\n",
        "\n",
        "            adv = adv.squeeze(1)\n",
        "            # normalize adv values\n",
        "            #adv = ( adv - torch.mean(adv) ) / ( torch.std(adv) + 1e-8)\n",
        "\n",
        "            # get policy actions probs for prob ratio & value prediction\n",
        "            pi, v = agent(obs)\n",
        "            # get the correct policy actions\n",
        "            pi = pi[range(minibatch_size),actions.long()]\n",
        "\n",
        "            # probaility ratio r_t(theta)\n",
        "            probability_ratio = pi / old_action_prob\n",
        "\n",
        "            # compute CPI\n",
        "            CPI = probability_ratio * adv\n",
        "            # compute clip*A_t\n",
        "            clip = torch.clamp(probability_ratio,1-eps,1+eps) * adv\n",
        "\n",
        "            # policy loss | take minimum\n",
        "            L_CLIP = torch.mean(torch.min(CPI, clip))\n",
        "\n",
        "            # value loss | mse\n",
        "            L_VF = torch.mean(torch.pow(v - v_target,2))\n",
        "\n",
        "            loss = - L_CLIP + c1 * L_VF\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcBwgvwXQ6MO",
        "outputId": "4ff6ea66-8d5c-4238-c22c-f3106d0dbdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -477.0\n",
            "Return: -500.0\n",
            "Return: -464.0\n",
            "Return: -500.0\n",
            "Return: -471.0\n",
            "Return: -500.0\n",
            "Return: -310.0\n",
            "Return: -449.0\n",
            "Return: -478.0\n",
            "Return: -500.0\n",
            "Return: -397.0\n",
            "Return: -345.0\n",
            "Return: -488.0\n",
            "Return: -500.0\n",
            "Return: -417.0\n",
            "Return: -459.0\n",
            "Return: -459.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n",
            "Return: -500.0\n"
          ]
        }
      ]
    }
  ]
}